# see https://github.com/sigoden/aichat/blob/main/config.example.yaml

# ---- llm ----
model: ollama:deepseek
temperature: null # Set default temperature parameter
top_p: null # Set default top-p parameter, range (0, 1)

# ---- behavior ----
stream: true # Controls whether to use the stream-style API.
save: true # Indicates whether to persist the message
keybindings: vi # Choose keybinding style (emacs, vi)
editor: null # Specifies the command used to edit input buffer or session. (e.g. vim, emacs, nano).
wrap: no # Controls text wrapping (no, auto, <max-width>)
wrap_code: false # Enables or disables wrapping of code blocks

# ---- function-calling ----
# Visit https://github.com/sigoden/llm-functions for setup instructions
function_calling: true # Enables or disables function calling (Globally).
mapping_tools: # Alias for a tool or toolset
  fs: "fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write"
use_tools: 'fs,web_search' # Which tools to use by default.

# ---- prelude ----
repl_prelude: null               # Set a default role or session for REPL mode (e.g. role:<name>, session:<name>, <session>:<role>)
cmd_prelude: null                # Set a default role or session for CMD mode (e.g. role:<name>, session:<name>, <session>:<role>)
agent_prelude: null              # Set a session to use when starting a agent (e.g. temp, default)

# ---- session ----
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: true
# Compress session when token count reaches or exceeds this threshold
compress_threshold: 4000
# Text prompt used for creating a concise summary of session message
summarize_prompt: "Summarize the discussion briefly in 200 words or less to use as a prompt for future context."
# Text prompt used for including the summary of the entire session
summary_prompt: "This is a summary of the chat history as a recap: "

# ---- RAG ----
# See [RAG-Guide](https://github.com/sigoden/aichat/wiki/RAG-Guide) for more details.
rag_embedding_model: null
rag_reranker_model: null # Specifies the rerank model to use
rag_top_k: 5 # Specifies the number of documents to retrieve
rag_chunk_size: null
rag_chunk_overlap: null
# Defines the query structure using variables like __CONTEXT__ and __INPUT__ to tailor searches to specific needs
rag_template: |
  Answer the query based on the context while respecting the rules. (user query, some textual context and rules, all inside xml tags)

  <context>
  __CONTEXT__
  </context>

  <rules>
  - If you don't know, just say so.
  - If you are not sure, ask for clarification.
  - Answer in the same language as the user query.
  - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.
  - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.
  - Answer directly and without using xml tags.
  </rules>

  <user_query>
  __INPUT__
  </user_query>

# Define document loaders to control how RAG and `.file`/`--file` load files of specific formats.
document_loaders:
  # You can add custom loaders using the following syntax:
  #   <file-extension>: <command-to-load-the-file>
  # Note: Use `$1` for input file and `$2` for output file. If `$2` is omitted, use stdout as output.
  pdf: "pdftotext $1 -" # Load .pdf file, see https://poppler.freedesktop.org to set up pdftotext
  docx: "pandoc --to plain $1" # Load .docx file, see https://pandoc.org to set up pandoc

# ---- appearance ----
highlight: true # Controls syntax highlighting
light_theme: false # env: AICHAT_LIGHT_THEME
# Custom REPL left/right prompts, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt for more details
left_prompt: "{color.green}{?session {?agent {agent}>}{session}{?role /}}{!session {?agent {agent}>}}{role}{?rag @{rag}}{color.cyan}{?session )}{!session >}{color.reset} "
right_prompt: "{color.purple}{?session {?consume_tokens {consume_tokens}({consume_percent}%)}{!consume_tokens {consume_tokens}}}{color.reset}"

# ---- misc ----
serve_addr: 127.0.0.1:8000
user_agent: auto # Set User-Agent HTTP header, use `auto` for aichat/<current-version>
save_shell_history: true
sync_models_url: https://raw.githubusercontent.com/sigoden/aichat/refs/heads/main/models.yaml

# ---- clients ----
clients:
  # All clients have the following configuration:
  # - type: xxxx
  #   name: xxxx                                      # Only use it to distinguish clients with the same client type. Optional
  #   models:
  #     - name: xxxx                                  # Chat model
  #       max_input_tokens: 100000
  #       supports_vision: true
  #       supports_function_calling: true
  #     - name: xxxx                                  # Embedding model
  #       type: embedding
  #       max_input_tokens: 200000
  #       max_tokens_per_chunk: 2000
  #       default_chunk_size: 1500
  #       max_batch_size: 100
  #     - name: xxxx                                  # Reranker model
  #       type: reranker
  #       max_input_tokens: 2048
  #   patch:                                          # Patch api
  #     chat_completions:                             # Api type, possible values: chat_completions, embeddings, and rerank
  #       <regex>:                                    # The regex to match model names, e.g. '.*' 'gpt-4o' 'gpt-4o|gpt-4-.*'
  #         url: ''                                   # Patch request url
  #         body:                                     # Patch request body
  #           <json>
  #         headers:                                  # Patch request headers
  #           <key>: <value>
  #   extra:
  #     proxy: socks5://127.0.0.1:1080                # Set proxy
  #     connect_timeout: 10                           # Set timeout in seconds for connect to api

  # See https://platform.openai.com/docs/quickstart
  - type: openai
    # api_key: $OPENAI_API_KEY

  # For any platform compatible with OpenAI's API
  - type: openai-compatible
    name: local
    api_base: http://localhost:8080/v1
    api_key: xxx
    models:
      - name: llama3.1
        max_input_tokens: 128000
        supports_function_calling: true
      - name: jina-embeddings-v2-base-en
        type: embedding
        default_chunk_size: 1500
        max_batch_size: 100
      - name: jina-reranker-v2-base-multilingual
        type: reranker

  # See https://ai.google.dev/docs
  - type: gemini
    api_base: https://generativelanguage.googleapis.com/v1beta
    patch:
      chat_completions:
        ".*":
          body:
            safetySettings:
              - category: HARM_CATEGORY_HARASSMENT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_HATE_SPEECH
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_DANGEROUS_CONTENT
                threshold: BLOCK_NONE

  # See https://docs.anthropic.com/claude/reference/getting-started-with-the-api
  - type: claude

  # See https://github.com/jmorganca/ollama
  - type: openai-compatible
    name: ollama
    api_base: http://localhost:11434/v1
    models:
      - name: llama3.2
        real_name: llama3.2:latest
        max_input_tokens: 131072
        supports_function_calling: true
      - name: mistral
        real_name: mistral:latest
        max_input_tokens: 32768
        supports_function_calling: true
      - name: mxbai
        real_name: mxbai-embed-large:latest
        type: embedding
        default_chunk_size: 1000
        max_batch_size: 100
        max_tokens_per_chunk: 512
        max_input_tokens: 200000
      - name: codellama
        real_name: codellama:13b
        max_input_tokens: 16384
      - name: qwen2.5-coder
        real_name: qwen2.5-coder:14b
        max_input_tokens: 32768
        supports_function_calling: true
      - name: deepseek
        real_name: deepseek-r1:8b
        max_input_tokens: 131072
        supports_reasoning: true
      - name: nomic-embed
        real_name: nomic-embed-text:latest
        type: embedding
        default_chunk_size: 1000
        max_batch_size: 50
      - name: qwen3
        real_name: qwen3:8b
        max_input_tokens: 40960
        supports_function_calling: true
        supports_reasoning: true
      - name: embed-gemma
        real_name: embeddinggemma:latest
        type: embedding
        default_chunk_size: 2048
        max_batch_size: 2048
